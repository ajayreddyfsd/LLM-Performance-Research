# LLM Response Research

This research explores the impact of emotional stimuli, persona-driven prompts, and zero-shot prompting on the response quality of multiple large language models (Gemini, OpenAI, Anthropic). Over 1824 tests were conducted using 152 astronomy MMLU questions, comparing the performance of these advanced prompting techniques with basic prompts. The findings highlight how context and personality influence LLM responses, providing valuable insights into optimizing AI interactions for better accuracy and engagement.

## Key Highlights
- 1824 tests across 152 MMLU questions in astronomy.
- Testing of emotional stimuli, persona-driven cues, and zero-shot CoT prompts.
- Comparison of multiple leading LLMs: Gemini, OpenAI, and Anthropic.

This project aims to push the boundaries of prompt engineering and deliver actionable insights for improving AI responses in complex tasks.
